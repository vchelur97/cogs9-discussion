{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6WEaFGqe8CH"
      },
      "outputs": [],
      "source": [
        "! pip install descartes\n",
        "! pip install geopandas\n",
        "! pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "RoY0J2dggmFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_data = gpd.datasets.get_path(\"nybb\")\n",
        "gdf = gpd.read_file(path_to_data)\n",
        "gdf"
      ],
      "metadata": {
        "id": "AXWjO979gsQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdf.plot(\"Shape_Area\", legend=True)"
      ],
      "metadata": {
        "id": "-CUAapI6hWxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdf[\"Population\"] = [1000, 2000, 8000, 300, 5000]"
      ],
      "metadata": {
        "id": "S4ai8-pphZ4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdf"
      ],
      "metadata": {
        "id": "B-N9sadYh13S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdf.plot(\"Population\", legend=True)"
      ],
      "metadata": {
        "id": "HbEj0cnLh9nS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdf['centroid'] = gdf.centroid\n",
        "ax = gdf.plot(\"Shape_Area\")\n",
        "gdf[\"centroid\"].plot(ax=ax, color=\"gray\", markersize=gdf[\"Population\"]/4)\n",
        "# gdf.plot(\"Shape_Area\", legend=True)"
      ],
      "metadata": {
        "id": "L7IDOzI-iCsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Borrowed from https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8\n",
        "from tqdm import tqdm\n",
        "\n",
        "texts = [\n",
        "    \"The future king is the prince\",\n",
        "    \"Daughter is the princess\",\n",
        "    \"Son is the prince\",\n",
        "    \"Only a man can be a king\",\n",
        "    \"Only a woman can be a queen\",\n",
        "    \"The princess will be a queen\",\n",
        "    \"Queen and king rule the realm\",\n",
        "    \"The prince is a strong man\",\n",
        "    \"The princess is a beautiful woman\",\n",
        "    \"The royal family is the king and queen and their children\",\n",
        "    \"Prince is only a boy now\",\n",
        "    \"A boy will be a man\"\n",
        "]"
      ],
      "metadata": {
        "id": "LlOhsGPfiHuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilities\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "def create_unique_word_dict(text:list) -> dict:\n",
        "    \"\"\"\n",
        "    A method that creates a dictionary where the keys are unique words\n",
        "    and key values are indices\n",
        "    \"\"\"\n",
        "    # Getting all the unique words from our text and sorting them alphabetically\n",
        "    words = list(set(text))\n",
        "    words.sort()\n",
        "\n",
        "    # Creating the dictionary for the unique words\n",
        "    unique_word_dict = {}\n",
        "    for i, word in enumerate(words):\n",
        "        unique_word_dict.update({\n",
        "            word: i\n",
        "        })\n",
        "\n",
        "    return unique_word_dict    \n",
        "\n",
        "def text_preprocessing(\n",
        "    text:list,\n",
        "    punctuations = r'''!()-[]{};:'\"\\,<>./?@#$%^&*_â€œ~''',\n",
        "    stop_words=['and', 'a', 'is', 'the', 'in', 'be', 'will']\n",
        "    )->list:\n",
        "    \"\"\"\n",
        "    A method to preproces text\n",
        "    \"\"\"\n",
        "    for x in text.lower(): \n",
        "        if x in punctuations: \n",
        "            text = text.replace(x, \"\")\n",
        "\n",
        "    # Removing words that have numbers in them\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "\n",
        "    # Removing digits\n",
        "    text = re.sub(r'[0-9]+', '', text)\n",
        "\n",
        "    # Cleaning the whitespaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Setting every word to lower\n",
        "    text = text.lower()\n",
        "\n",
        "    # Converting all our text to a list \n",
        "    text = text.split(' ')\n",
        "\n",
        "    # Droping empty strings\n",
        "    text = [x for x in text if x!='']\n",
        "\n",
        "    # Droping stop words\n",
        "    text = [x for x in text if x not in stop_words]\n",
        "\n",
        "    return text\n",
        "\n",
        "# Functions to find the most similar word \n",
        "def euclidean(vec1:np.array, vec2:np.array) -> float:\n",
        "    \"\"\"\n",
        "    A function to calculate the euclidean distance between two vectors\n",
        "    \"\"\"\n",
        "    return np.sqrt(np.sum((vec1 - vec2)**2))\n",
        "\n",
        "def find_similar(word:str, embedding_dict:dict, top_n=10)->list:\n",
        "    \"\"\"\n",
        "    A method to find the most similar word based on the learnt embeddings\n",
        "    \"\"\"\n",
        "    dist_dict = {}\n",
        "    word_vector = embedding_dict.get(word, [])\n",
        "    if len(word_vector) > 0:\n",
        "        for key, value in embedding_dict.items():\n",
        "            if key!=word:\n",
        "                dist = euclidean(word_vector, value)\n",
        "                dist_dict.update({\n",
        "                    key: dist\n",
        "                })\n",
        "\n",
        "        return sorted(dist_dict.items(), key=lambda x: x[1])[0:top_n]   "
      ],
      "metadata": {
        "id": "9E-HXK6Dq7si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the window for context\n",
        "window = 2\n",
        "\n",
        "# Creating a placeholder for the scanning of the word list\n",
        "word_lists = []\n",
        "all_text = []\n",
        "\n",
        "for text in texts:\n",
        "\n",
        "    # Cleaning the text\n",
        "    text = text_preprocessing(text)\n",
        "\n",
        "    # Appending to the all text list\n",
        "    all_text += text \n",
        "\n",
        "    # Creating a context dictionary\n",
        "    for i, word in enumerate(text):\n",
        "        for w in range(window):\n",
        "            # Getting the context that is ahead by *window* words\n",
        "            if i + 1 + w < len(text): \n",
        "                word_lists.append([word] + [text[(i + 1 + w)]])\n",
        "            # Getting the context that is behind by *window* words    \n",
        "            if i - w - 1 >= 0:\n",
        "                word_lists.append([word] + [text[(i - w - 1)]])\n",
        "\n",
        "unique_word_dict = create_unique_word_dict(all_text)\n",
        "unique_word_dict"
      ],
      "metadata": {
        "id": "EADsfpY2rAcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse\n",
        "import numpy as np\n",
        "\n",
        "# Defining the number of features (unique words)\n",
        "n_words = len(unique_word_dict)\n",
        "\n",
        "# Getting all the unique words \n",
        "words = list(unique_word_dict.keys())\n",
        "\n",
        "# Creating the X and Y matrices using one hot encoding\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "for i, word_list in tqdm(enumerate(word_lists)):\n",
        "    # Getting the indices\n",
        "    main_word_index = unique_word_dict.get(word_list[0])\n",
        "    context_word_index = unique_word_dict.get(word_list[1])\n",
        "\n",
        "    # Creating the placeholders   \n",
        "    X_row = np.zeros(n_words)\n",
        "    Y_row = np.zeros(n_words)\n",
        "\n",
        "    # One hot encoding the main word\n",
        "    X_row[main_word_index] = 1\n",
        "\n",
        "    # One hot encoding the Y matrix words \n",
        "    Y_row[context_word_index] = 1\n",
        "\n",
        "    # Appending to the main matrices\n",
        "    X.append(X_row)\n",
        "    Y.append(Y_row)\n",
        "\n",
        "# Converting the matrices into an array\n",
        "X = np.asarray(X)\n",
        "Y = np.asarray(Y)"
      ],
      "metadata": {
        "id": "3St-D8ndrxkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep learning: \n",
        "from keras import Input, Model\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Defining the size of the embedding\n",
        "embed_size = 2\n",
        "\n",
        "# Defining the neural network\n",
        "inp = Input(shape=(X.shape[1],))\n",
        "x = Dense(units=embed_size, activation='linear')(inp)\n",
        "x = Dense(units=Y.shape[1], activation='softmax')(x)\n",
        "model = Model(inputs=inp, outputs=x)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
        "\n",
        "# Optimizing the network weights\n",
        "model.fit(\n",
        "    x=X, \n",
        "    y=Y, \n",
        "    batch_size=256,\n",
        "    epochs=1000\n",
        "    )\n",
        "\n",
        "# Obtaining the weights from the neural network. \n",
        "# These are the so called word embeddings\n",
        "\n",
        "# The input layer \n",
        "weights = model.get_weights()[0]\n",
        "\n",
        "# Creating a dictionary to store the embeddings in. The key is a unique word and \n",
        "# the value is the numeric vector\n",
        "embedding_dict = {}\n",
        "for word in words: \n",
        "    embedding_dict.update({\n",
        "        word: weights[unique_word_dict.get(word)]\n",
        "        })"
      ],
      "metadata": {
        "id": "ONuat0F-r1IL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for word in list(unique_word_dict.keys()):\n",
        "    coord = embedding_dict.get(word)\n",
        "    plt.scatter(coord[0], coord[1])\n",
        "    plt.annotate(word, (coord[0], coord[1]))"
      ],
      "metadata": {
        "id": "Rzu_zk3Nr3Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MUOD6mlKtov6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}